# Huggingface.co models to use for the llama2 chatbot
[
    {
        # Poor: High mem footprint >20GB (GPU needed but still slow)
        "format": "hf",  # Huggingface Library
        "name": "meta-llama/Llama-2-7b-chat-hf"   
    },
    {
        # Medium/Good: Very fast and low mem footprint ~3GB (CPU/GPU)
        "format": "ggml",  # GGML Library
        "name": "TheBloke/Llama-2-7B-Chat-GGML",  
        "file": "llama-2-7b-chat.ggmlv3.q4_K_M.bin"
    },
    {
        # Best: Very low mem footprint and emojies! (GPU needed)
        "format": "gptq",  # GPTQ Library
        "name": "TheBloke/Llama-2-7b-Chat-GPTQ"   
    },
    {
        # Poor: High mem footprint >20GB (GPU needed but still slow)
        # This requires that you run 'main train' to generate the model
        # But LLama v2 is just incompatible with the HF library
        "format": "gptq",  # Huggingface Library
        "name": "olafrv/Llama-2-7b-chat-hf-trained",
        "offline": True
    },
]